# Kaggle Competitions Course

##Kaggle Competitions Course - Learn in a fast way how to compete at Kaggle

The main objective of this course is to make your life easy and put you up and running as fast as you can.

For sure, it does not mean that you will become an experient data science, it will take a lot of years, but my idea is to help you start in a really fun way.

##Why Kaggle Competitions?

Because it rocks and you will love it. Using this amazing website you can learn, compete and maybe make a lot of money.

##How long does it take to do the course?

3 hours

##How much does it cost?

It is a free course.

##Is it based on what?

This course is based on Prof. Caio Moreno personal experiences, on Federico Castaneda classes at U-TAD, Kaggle Tutorials, Google and on this website free course https://www.dataquest.io/course/kaggle-competitions.

## Dataset 

We will use the GiveMeSomeCredit dataset for this course (https://www.kaggle.com/c/GiveMeSomeCredit).

###Links:

Kaggle Tutorials<BR>
https://www.kaggle.com/wiki/Start<BR>

10 Steps to Success in Kaggle Data Science Competitions<BR>
http://www.kdnuggets.com/2015/03/10-steps-success-kaggle-data-science-competitions.html<BR>

What You Can Learn From Kaggle's Top 10 Data Scientists<BR> 
http://readwrite.com/2012/04/12/what-you-can-learn-from-kaggle<BR>

Kaggle Competitions<BR>
https://www.dataquest.io/course/kaggle-competitions<BR>

Kaggle in Class<BR>
https://inclass.kaggle.com/<BR>

The caret Package<BR>
http://topepo.github.io/caret/<BR>

The caret Package - The Model List<BR>
http://topepo.github.io/caret/modelList.html<BR>

Should I do a PhD?<BR>
https://www.kaggle.com/owise83/2013-american-community-survey/should-i-do-a-phd<BR>

I did a PhD! Where to Look for a Job?<BR>
https://www.kaggle.com/owise83/2013-american-community-survey/i-did-a-phd-where-to-look-for-a-job<BR>

## Comparación algoritmos aprendizaje supervisado

Dependiendo de la métrica elegida los diferentes algoritmos pueden variar.<BR>

[Rich Caruana, ICML 2006] evaluaron diversos algoritmos en 11 problemas de clasificación.<BR>

Sin una calibración óptima, bagged trees, random forests y neural networks proporcionan los mejores resultado..<BR>

De media boosted stumps, single decision trees, logistic regression y naive bayes no son competitivos entre los mejores métodos.<BR>

La calibración mejora bastante el rendimiento de los boosted trees, SVM y naive bayes. En el caso de los random forests lo mejora levemente.<BR>

## Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? 

We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large- scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).


http://jmlr.org/papers/v15/delgado14a.html
